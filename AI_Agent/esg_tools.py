# esg_tools.py
import os
import re
import json
import requests
import yfinance as yf
import PyPDF2
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional

# --- (1) Tool: Ä‘á»c PDF ---
import os
import glob
import PyPDF2

def extract_text_from_pdf(path: str) -> str:
    """
    Cho phÃ©p nháº­p:
    - Ä‘Æ°á»ng dáº«n file .pdf
    - hoáº·c thÆ° má»¥c chá»©a pdf (tá»± chá»n file pdf má»›i nháº¥t)
    """
    path = path.strip('"').strip()

    # Náº¿u lÃ  thÆ° má»¥c -> tÃ¬m pdf
    if os.path.isdir(path):
        pdfs = sorted(
            glob.glob(os.path.join(path, "*.pdf")),
            key=os.path.getmtime,
            reverse=True
        )
        if not pdfs:
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file PDF trong thÆ° má»¥c: {path}")
        path = pdfs[0]  # láº¥y file má»›i nháº¥t

    # Náº¿u khÃ´ng pháº£i file tá»“n táº¡i
    if not os.path.exists(path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {path}")

    # Náº¿u khÃ´ng pháº£i pdf
    if not path.lower().endswith(".pdf"):
        raise ValueError(f"File khÃ´ng pháº£i PDF: {path}")

    text = ""
    with open(path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            text += (page.extract_text() or "") + "\n"
    return text

# --- (2) Tool: hard data tá»« yfinance ---
def get_basic_esg_data(ticker: str) -> str:
    """
    Láº¥y dá»¯ liá»‡u ESG cÆ¡ báº£n tá»« Yahoo Finance náº¿u cÃ³.
    """
    stock = yf.Ticker(ticker)
    try:
        esg_data = getattr(stock, "sustainability", None)
        if esg_data is None:
            return "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u ESG cÃ³ sáºµn trÃªn Yahoo Finance."
        return esg_data.to_string()
    except Exception as e:
        return f"Lá»—i yfinance: {str(e)}"


from typing import List, Dict
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import json


def google_search_cse(query: str, num_results: int = 5) -> List[Dict[str, str]]:
    """
    TÃ¬m Google qua Custom Search JSON API.
    - ÄÃ£ hardcode API key & CSE ID
    - CÃ³ debug lá»—i 403 chi tiáº¿t
    - CÃ³ fallback Ä‘á»ƒ khÃ´ng lÃ m crash pipeline
    """

    # ðŸ”´ API KEY & CSE ID (Ä‘Ãºng cÃ¡i báº¡n Ä‘ang dÃ¹ng)
    api_key = "AIzaSyCfTpSXdC3LfX-CIlHscAL8NRzWyAaknlI"
    cse_id  = "f762af7348cde4afd"

    if not api_key or not cse_id:
        raise EnvironmentError("Thiáº¿u GOOGLE_CSE_API_KEY hoáº·c GOOGLE_CSE_ID.")

    try:
        service = build(
            "customsearch",
            "v1",
            developerKey=api_key,
            cache_discovery=False  # trÃ¡nh lá»—i cache láº·t váº·t trÃªn Windows
        )

        res = service.cse().list(
            q=query,
            cx=cse_id,
            num=min(num_results, 10)
        ).execute()

        items = res.get("items", [])
        return [
            {
                "title": it.get("title", ""),
                "link": it.get("link", ""),
                "snippet": it.get("snippet", "")
            }
            for it in items
        ]

    except HttpError as e:
        # ðŸ”Ž In lá»—i CHI TIáº¾T tá»« Google (ráº¥t quan trá»ng)
        print("âŒ Google CSE HttpError")
        try:
            error_detail = json.loads(e.content.decode("utf-8"))
            print(json.dumps(error_detail, indent=2, ensure_ascii=False))
        except Exception:
            print(str(e))

        # â›‘ï¸ Fallback: khÃ´ng cho pipeline cháº¿t
        return []

    except Exception as e:
        print("âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi gá»i Google CSE:", str(e))
        return []





# --- (4) Tool: láº¥y ná»™i dung bÃ i bÃ¡o (webpage -> text) ---
def fetch_url_text(url: str, timeout: int = 15) -> str:
    """
    Láº¥y text thÃ´ tá»« URL (tin tá»©c/press release). (CÃ³ thá»ƒ fail náº¿u trang cháº·n bot)
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    r = requests.get(url, headers=headers, timeout=timeout)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    # bá» script/style
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    text = soup.get_text(separator=" ")
    text = re.sub(r"\s+", " ", text).strip()
    return text


# --- (5) Tool: Soft data classifier (FinBERT-ESG) ---
class FinBertESGTool:
    """
    Load 1 láº§n Ä‘á»ƒ dÃ¹ng láº¡i (Ä‘á»¡ cháº­m).
    """
    def __init__(self, model_name: str = "yiyanghkust/finbert-esg"):
        from transformers import pipeline
        self.classifier = pipeline("text-classification", model=model_name)

    def analyze(self, text: str) -> Any:
        snippet = text[:512]
        return self.classifier(snippet)


# --- (6) Tool: gom soft data tá»« Google Search -> fetch -> FinBERT ---
def collect_esg_news_signals(
    company_name: str,
    finbert_tool: FinBertESGTool,
    num_articles: int = 5
) -> Dict[str, Any]:
    """
    TÃ¬m tin tá»©c liÃªn quan ESG + phÃ¢n loáº¡i nhanh theo FinBERT-ESG.
    """
    query = f'{company_name} ESG controversy OR scandal OR labor OR emissions OR bribery OR data breach'
    results = google_search_cse(query, num_results=num_articles)

    analyzed = []
    for r in results:
        url = r["link"]
        try:
            page_text = fetch_url_text(url)
            pred = finbert_tool.analyze(page_text)
            analyzed.append({
                "title": r["title"],
                "link": url,
                "snippet": r["snippet"],
                "finbert_esg": pred
            })
        except Exception as e:
            analyzed.append({
                "title": r["title"],
                "link": url,
                "snippet": r["snippet"],
                "finbert_esg": None,
                "error": str(e)
            })

    return {
        "query": query,
        "results": analyzed
    }
